{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06aa19b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "366e8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43a659",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "1. Randomized CV - Samples a fixed number of random parameter combinations from distributions.\n",
    "2. Gridsearch CV - Exhaustively searches all possible combinations in a fixed grid.\n",
    "\n",
    "Use the randomized CV on the following models \n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- Support Vector Classifier\n",
    "- K-Nearest Neighbors\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- Naive Bayes\n",
    "\n",
    "Use GridSearchCV on the top 2-3 models to \"zoom in\" on the best parameters found by the random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3020203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(), \n",
    "    'Random Forest': RandomForestClassifier(), \n",
    "    'Support Vector Classifier': SVC(), \n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(), \n",
    "    'XGBoost': XGBClassifier(), \n",
    "    'LightGBM': LGBMClassifier(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f98a1",
   "metadata": {},
   "source": [
    "Model and\tMost Important Hyperparameters to Tune\n",
    "\n",
    "- Logistic Regression: C (Inverse regularization strength), penalty ('l1', 'l2', 'elasticnet'), solver ('liblinear', 'saga').\n",
    "- Random Forest: n_estimators (Number of trees), max_depth, min_samples_split, max_features ('sqrt', 'log2').\n",
    "- SVC: C (Regularization), kernel ('linear', 'rbf', 'poly'), gamma (Kernel coefficient).\n",
    "- K-Nearest Neighbors: n_neighbors (Number of neighbors), weights ('uniform', 'distance'), metric ('euclidean', 'manhattan').\n",
    "- Gradient Boosting: n_estimators, learning_rate (Shrinkage), max_depth, subsample (Fraction of samples for each tree).\n",
    "- XGBoost: n_estimators, learning_rate (eta), max_depth, min_child_weight, gamma, subsample, colsample_bytree.\n",
    "- LightGBM: num_leaves (Max leaves), max_depth, learning_rate, n_estimators, min_child_samples.\n",
    "- Naive Bayes: var_smoothing (Portion of the largest variance added to variances for calculation stability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85759ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 50, 80, 110, 140]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(20, 150, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b63a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter\n",
    "\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': np.logspace(-3,3,8),\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': list(range(2, 15, 2)),\n",
    "        'min_samples_leaf': list(range(1, 10, 2)),\n",
    "        \"criterion\": [\"gini\", \"entropy\"]\n",
    "    },\n",
    "    'Support Vector Classifier': {\n",
    "        'C': [0.1, 1, 10, 100, 1000], \n",
    "\t\t'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "\t\t'kernel': ['rbf', 'linear']\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'n_neighbors': list(range(2, 25, 2)),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200, 300, 500, 700, 800],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1, 0.2, 0.3],\n",
    "        'max_depth': list(range(3, 10, 2)),\n",
    "        'subsample': [0.6, 0.8, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200, 300, 500, 700, 800],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1, 0.2, 0.3],\n",
    "        'max_depth': list(range(3, 10, 2)),\n",
    "        'gamma': [0, 0.1, 0.5, 1],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'objective' : ['binnary'],\n",
    "        'n_estimators': [50, 100, 200, 300, 500, 700, 800],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1, 0.2],\n",
    "        'num_leaves': list(range(20, 150, 30)),\n",
    "        'max_depth': [-1, 10, 20],\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'var_smoothing': np.logspace(0,-9, num=10)\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
